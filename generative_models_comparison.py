# -*- coding: utf-8 -*-
"""generative_models_comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oLhq722q75eNgJ2S7qWtygw13Crfk08d

# MNIST 手寫數字生成模型比較：VAE、GAN、cGAN 與 Diffusion

本專案旨在實作並比較四種主流的生成模型在 MNIST 手寫數字生成任務上的表現。我們將逐一建構、訓練並評估以下模型：

1.  **變分自動編碼器 (Variational Autoencoder, VAE)**
2.  **生成對抗網路 (Generative Adversarial Network, GAN)**
3.  **條件生成對抗網路 (Conditional GAN, cGAN)**
4.  **去噪擴散機率模型 (Denoising Diffusion Probabilistic Model, DDPM)**

我們將遵循指定的作業要求，對各模型在**清晰度、穩定性、可控性、效率**四個維度上進行分析與比較。

## 1. 環境設定與參數配置

首先，我們導入所有必要的函式庫，並設定好訓練所需的超參數、設備與隨機種子以確保實驗的可重複性。
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torchvision.utils import save_image, make_grid
from torchvision.models.inception import inception_v3
from torch.nn.functional import adaptive_avg_pool2d
from scipy.stats import entropy

import numpy as np
import matplotlib.pyplot as plt
import os
import gc
from tqdm import tqdm
import time
from datetime import datetime

# --- 參數設定 ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

SEED = 42
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)

BATCH_SIZE = 128
EPOCHS = 30  # 可根據需求增加至 50+
IMG_SIZE = 28  # <--- 修正點：還原回 28x28
IMG_SHAPE = (1, IMG_SIZE, IMG_SIZE)
LATENT_DIM = 100  # GAN/cGAN 的噪聲維度
VAE_LATENT_DIM = 20  # VAE 的潛在空間維度
N_CLASSES = 10

# --- 學習率 ---
LR_VAE = 1e-3
LR_GAN = 2e-4
LR_DDPM = 1e-3

# --- 建立輸出資料夾 ---
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
os.makedirs("outputs/images/vae", exist_ok=True)
os.makedirs("outputs/images/gan", exist_ok=True)
os.makedirs("outputs/images/cgan", exist_ok=True)
os.makedirs("outputs/images/ddpm", exist_ok=True)
os.makedirs("outputs/images/comparison", exist_ok=True)
os.makedirs("outputs/checkpoints", exist_ok=True)
os.makedirs("outputs/metrics", exist_ok=True)

# Early stopping parameters
PATIENCE = 5
MIN_DELTA = 1e-4

# GPU memory optimization
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.backends.cudnn.benchmark = True

"""## 2. 資料載入

使用 `torchvision` 下載並準備 MNIST 資料集。我們將圖片標準化至 `[-1, 1]` 範圍，這對 GAN 的訓練特別有幫助。
"""

transform = transforms.Compose(
    [
        # <--- 修正點：移除 Resize 操作
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),  # 將像素值從 [0, 1] 轉換到 [-1, 1]
    ]
)

mnist_dataset = datasets.MNIST(
    root="./data", train=True, download=True, transform=transform
)
dataloader = DataLoader(mnist_dataset, batch_size=BATCH_SIZE, shuffle=True)


# --- 輔助函數：顯示圖片 ---
def show_images(imgs, title="Generated Images", save_path=None):
    imgs = imgs.cpu().detach()
    imgs = (imgs + 1) / 2  # 將像素值從 [-1, 1] 轉換回 [0, 1]
    grid = make_grid(imgs, nrow=10, normalize=True)

    plt.figure(figsize=(12, 6))
    plt.imshow(grid.permute(1, 2, 0))
    plt.title(title)
    plt.axis("off")

    if save_path:
        plt.savefig(save_path, bbox_inches="tight", dpi=150)
        print(f"Saved image to {save_path}")

    plt.show()


# --- 早停機制類別 ---
class EarlyStopping:
    def __init__(self, patience=5, min_delta=1e-4, mode="min"):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif self.mode == "min":
            if score < self.best_score - self.min_delta:
                self.best_score = score
                self.counter = 0
            else:
                self.counter += 1
        else:  # mode == 'max'
            if score > self.best_score + self.min_delta:
                self.best_score = score
                self.counter = 0
            else:
                self.counter += 1

        if self.counter >= self.patience:
            self.early_stop = True

        return self.early_stop


# --- 評估指標函數 ---
def calculate_inception_score(imgs, batch_size=32, splits=10):
    """計算 Inception Score (IS)"""
    try:
        # 將圖片從 1 通道轉換為 3 通道 (RGB)
        if imgs.shape[1] == 1:
            imgs = imgs.repeat(1, 3, 1, 1)

        # 調整圖片大小到 299x299 (Inception v3 的輸入大小)
        imgs = F.interpolate(
            imgs, size=(299, 299), mode="bilinear", align_corners=False
        )

        # 載入預訓練的 Inception v3 模型
        inception_model = inception_v3(pretrained=True, transform_input=False).to(
            DEVICE
        )
        inception_model.eval()

        def get_pred(x):
            with torch.no_grad():
                x = inception_model(x)
                return F.softmax(x, dim=1).cpu().numpy()

        # 分批計算
        preds = []
        for i in range(0, len(imgs), batch_size):
            batch = imgs[i : i + batch_size].to(DEVICE)
            pred = get_pred(batch)
            preds.append(pred)
            clear_gpu_memory()

        preds = np.concatenate(preds, axis=0)

        # 計算 Inception Score
        scores = []
        for i in range(splits):
            part = preds[i * len(preds) // splits : (i + 1) * len(preds) // splits]
            kl_div = part * (
                np.log(part + 1e-16)
                - np.log(np.expand_dims(np.mean(part, axis=0), 0) + 1e-16)
            )
            kl_div = np.mean(np.sum(kl_div, axis=1))
            scores.append(np.exp(kl_div))

        return np.mean(scores), np.std(scores)
    except Exception as e:
        print(f"Error calculating IS: {e}")
        return 1.0, 0.0


def calculate_fid_score(real_imgs, fake_imgs, batch_size=16):
    """簡化版 FID Score 計算"""
    try:

        def get_features(imgs):
            if imgs.shape[1] == 1:
                imgs = imgs.repeat(1, 3, 1, 1)
            imgs = F.interpolate(
                imgs, size=(299, 299), mode="bilinear", align_corners=False
            )

            inception_model = inception_v3(pretrained=True, transform_input=False).to(
                DEVICE
            )
            inception_model.eval()

            features = []
            for i in range(0, len(imgs), batch_size):
                batch = imgs[i : i + batch_size].to(DEVICE)
                with torch.no_grad():
                    feat = inception_model.avgpool(inception_model.Mixed_7c(batch))
                    feat = adaptive_avg_pool2d(feat, (1, 1))
                    features.append(feat.view(feat.size(0), -1).cpu())
                clear_gpu_memory()

            return torch.cat(features, dim=0)

        real_features = get_features(real_imgs)
        fake_features = get_features(fake_imgs)

        # 計算均值差異 (簡化版 FID)
        mu_real = torch.mean(real_features, dim=0)
        mu_fake = torch.mean(fake_features, dim=0)

        fid_score = torch.norm(mu_real - mu_fake).item()
        return fid_score
    except Exception as e:
        print(f"Error calculating FID: {e}")
        return 100.0


# --- GPU 記憶體優化函數 ---
def clear_gpu_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


"""## 3. 模型一：變分自動編碼器 (VAE)

VAE 包含一個編碼器 (Encoder) 和一個解碼器 (Decoder)。編碼器將輸入影像壓縮成一個機率性的潛在空間分佈（均值 μ 與對數方差 logσ²），而解碼器則從潛在空間中採樣並還原影像。
"""


# VAE 模型定義
class VAE(nn.Module):
    def __init__(
        self, input_dim=IMG_SIZE * IMG_SIZE, hidden_dim=400, latent_dim=VAE_LATENT_DIM
    ):
        super(VAE, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Tanh(),  # 輸出範圍為 [-1, 1]，匹配資料
        )

    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_log_var(h)

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        mu, log_var = self.encode(x.view(-1, IMG_SIZE * IMG_SIZE))
        z = self.reparameterize(mu, log_var)
        return self.decode(z), mu, log_var


# VAE 損失函數
def vae_loss_function(recon_x, x, mu, log_var):
    BCE = F.mse_loss(recon_x, x.view(-1, IMG_SIZE * IMG_SIZE), reduction="sum")
    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    return BCE + KLD


# VAE 訓練
def train_vae():
    model = VAE().to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR_VAE)
    early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA)

    print("--- Starting VAE Training ---")
    train_losses = []

    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0

        # Progress bar for batches
        pbar = tqdm(dataloader, desc=f"VAE Epoch {epoch + 1}/{EPOCHS}")
        for batch_idx, (data, _) in enumerate(pbar):
            data = data.to(DEVICE)
            optimizer.zero_grad()
            recon_batch, mu, log_var = model(data)
            loss = vae_loss_function(recon_batch, data, mu, log_var)
            loss.backward()
            train_loss += loss.item()
            optimizer.step()

            # Update progress bar
            pbar.set_postfix({"Loss": f"{loss.item():.4f}"})

            # GPU memory management
            if batch_idx % 50 == 0:
                clear_gpu_memory()

        avg_loss = train_loss / len(dataloader.dataset)
        train_losses.append(avg_loss)
        print(f"====> Epoch: {epoch + 1} Average loss: {avg_loss:.4f}")

        # Early stopping
        if early_stopping(avg_loss):
            print(f"Early stopping at epoch {epoch + 1}")
            break

    # Save model and training history
    torch.save(
        {
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "train_losses": train_losses,
            "epoch": epoch,
        },
        "outputs/checkpoints/vae.pth",
    )

    print("--- VAE Training Finished ---")
    clear_gpu_memory()
    return model


# 執行 VAE 訓練
vae_model = train_vae()

"""## 4. 模型二：生成對抗網路 (GAN)

GAN 由一個生成器 (Generator) 和一個判別器 (Discriminator) 組成。生成器負責從隨機噪聲生成假影像，判別器則負責判斷輸入的影像是真是假。兩者在對抗中共同進步。
"""


# GAN 模型定義
class Generator_GAN(nn.Module):
    def __init__(self):
        super(Generator_GAN, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(LATENT_DIM, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(1024, int(np.prod(IMG_SHAPE))),
            nn.Tanh(),
        )

    def forward(self, z):
        img = self.model(z)
        img = img.view(img.size(0), *IMG_SHAPE)
        return img


class Discriminator_GAN(nn.Module):
    def __init__(self):
        super(Discriminator_GAN, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(int(np.prod(IMG_SHAPE)), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        )

    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)
        return validity


# GAN 訓練
def train_gan():
    generator = Generator_GAN().to(DEVICE)
    discriminator = Discriminator_GAN().to(DEVICE)
    adversarial_loss = nn.BCELoss()

    optimizer_G = optim.Adam(generator.parameters(), lr=LR_GAN, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(discriminator.parameters(), lr=LR_GAN, betas=(0.5, 0.999))

    # Early stopping based on generator loss
    early_stopping = EarlyStopping(
        patience=PATIENCE * 2, min_delta=MIN_DELTA, mode="min"
    )

    print("--- Starting GAN Training ---")
    d_losses, g_losses = [], []

    for epoch in range(EPOCHS):
        epoch_d_loss, epoch_g_loss = 0, 0

        # Progress bar for batches
        pbar = tqdm(dataloader, desc=f"GAN Epoch {epoch + 1}/{EPOCHS}")
        for i, (imgs, _) in enumerate(pbar):
            real_imgs = imgs.to(DEVICE)
            valid = torch.ones(imgs.size(0), 1, device=DEVICE, requires_grad=False)
            fake = torch.zeros(imgs.size(0), 1, device=DEVICE, requires_grad=False)

            # --- 訓練生成器 ---
            optimizer_G.zero_grad()
            z = torch.randn(imgs.size(0), LATENT_DIM, device=DEVICE)
            gen_imgs = generator(z)
            g_loss = adversarial_loss(discriminator(gen_imgs), valid)
            g_loss.backward()
            optimizer_G.step()

            # --- 訓練判別器 ---
            optimizer_D.zero_grad()
            real_loss = adversarial_loss(discriminator(real_imgs), valid)
            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)
            d_loss = (real_loss + fake_loss) / 2
            d_loss.backward()
            optimizer_D.step()

            epoch_d_loss += d_loss.item()
            epoch_g_loss += g_loss.item()

            # Update progress bar
            pbar.set_postfix(
                {"D_loss": f"{d_loss.item():.4f}", "G_loss": f"{g_loss.item():.4f}"}
            )

            # GPU memory management
            if i % 50 == 0:
                clear_gpu_memory()

        avg_d_loss = epoch_d_loss / len(dataloader)
        avg_g_loss = epoch_g_loss / len(dataloader)
        d_losses.append(avg_d_loss)
        g_losses.append(avg_g_loss)

        print(
            f"[Epoch {epoch + 1}/{EPOCHS}] [D loss: {avg_d_loss:.4f}] [G loss: {avg_g_loss:.4f}]"
        )

        # Early stopping based on generator loss
        if early_stopping(avg_g_loss):
            print(f"Early stopping at epoch {epoch + 1}")
            break

    # Save models and training history
    torch.save(
        {
            "generator_state_dict": generator.state_dict(),
            "discriminator_state_dict": discriminator.state_dict(),
            "optimizer_G_state_dict": optimizer_G.state_dict(),
            "optimizer_D_state_dict": optimizer_D.state_dict(),
            "d_losses": d_losses,
            "g_losses": g_losses,
            "epoch": epoch,
        },
        "outputs/checkpoints/gan_generator.pth",
    )

    print("--- GAN Training Finished ---")
    clear_gpu_memory()
    return generator


# 執行 GAN 訓練
gan_generator = train_gan()

"""## 5. 模型三：條件生成對抗網路 (cGAN)

cGAN 是 GAN 的擴展，它允許我們透過提供額外的條件資訊（例如類別標籤）來控制生成影像的內容。生成器和判別器都會接收這個條件資訊。
"""


# cGAN 模型定義
class Generator_cGAN(nn.Module):
    def __init__(self):
        super(Generator_cGAN, self).__init__()
        self.label_emb = nn.Embedding(N_CLASSES, N_CLASSES)
        self.model = nn.Sequential(
            nn.Linear(LATENT_DIM + N_CLASSES, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(1024, int(np.prod(IMG_SHAPE))),
            nn.Tanh(),
        )

    def forward(self, z, labels):
        c = self.label_emb(labels)
        x = torch.cat([z, c], 1)
        img = self.model(x)
        return img.view(img.size(0), *IMG_SHAPE)


class Discriminator_cGAN(nn.Module):
    def __init__(self):
        super(Discriminator_cGAN, self).__init__()
        self.label_emb = nn.Embedding(N_CLASSES, N_CLASSES)
        self.model = nn.Sequential(
            nn.Linear(int(np.prod(IMG_SHAPE)) + N_CLASSES, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        )

    def forward(self, img, labels):
        c = self.label_emb(labels)
        x = torch.cat([img.view(img.size(0), -1), c], 1)
        validity = self.model(x)
        return validity


# cGAN 訓練
def train_cgan():
    generator = Generator_cGAN().to(DEVICE)
    discriminator = Discriminator_cGAN().to(DEVICE)
    adversarial_loss = nn.BCELoss()

    optimizer_G = optim.Adam(generator.parameters(), lr=LR_GAN, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(discriminator.parameters(), lr=LR_GAN, betas=(0.5, 0.999))

    # Early stopping based on generator loss
    early_stopping = EarlyStopping(
        patience=PATIENCE * 2, min_delta=MIN_DELTA, mode="min"
    )

    print("--- Starting cGAN Training ---")
    d_losses, g_losses = [], []

    for epoch in range(EPOCHS):
        epoch_d_loss, epoch_g_loss = 0, 0

        # Progress bar for batches
        pbar = tqdm(dataloader, desc=f"cGAN Epoch {epoch + 1}/{EPOCHS}")
        for i, (imgs, labels) in enumerate(pbar):
            real_imgs = imgs.to(DEVICE)
            labels = labels.to(DEVICE)

            # Label smoothing for real samples
            valid = torch.full(
                (imgs.size(0), 1), 0.9, device=DEVICE, requires_grad=False
            )
            fake = torch.zeros(imgs.size(0), 1, device=DEVICE, requires_grad=False)

            # --- 訓練生成器 ---
            optimizer_G.zero_grad()
            z = torch.randn(imgs.size(0), LATENT_DIM, device=DEVICE)
            gen_labels = torch.randint(0, N_CLASSES, (imgs.size(0),), device=DEVICE)
            gen_imgs = generator(z, gen_labels)
            # We use non-smoothed labels for generator loss
            valid_for_g = torch.ones(
                imgs.size(0), 1, device=DEVICE, requires_grad=False
            )
            g_loss = adversarial_loss(discriminator(gen_imgs, gen_labels), valid_for_g)
            g_loss.backward()
            optimizer_G.step()

            # --- 訓練判別器 ---
            optimizer_D.zero_grad()
            real_loss = adversarial_loss(discriminator(real_imgs, labels), valid)
            fake_loss = adversarial_loss(
                discriminator(gen_imgs.detach(), gen_labels), fake
            )
            d_loss = (real_loss + fake_loss) / 2
            d_loss.backward()
            optimizer_D.step()

            epoch_d_loss += d_loss.item()
            epoch_g_loss += g_loss.item()

            # Update progress bar
            pbar.set_postfix(
                {"D_loss": f"{d_loss.item():.4f}", "G_loss": f"{g_loss.item():.4f}"}
            )

            # GPU memory management
            if i % 50 == 0:
                clear_gpu_memory()

        avg_d_loss = epoch_d_loss / len(dataloader)
        avg_g_loss = epoch_g_loss / len(dataloader)
        d_losses.append(avg_d_loss)
        g_losses.append(avg_g_loss)

        print(
            f"[Epoch {epoch + 1}/{EPOCHS}] [D loss: {avg_d_loss:.4f}] [G loss: {avg_g_loss:.4f}]"
        )

        # Early stopping based on generator loss
        if early_stopping(avg_g_loss):
            print(f"Early stopping at epoch {epoch + 1}")
            break

    # Save models and training history
    torch.save(
        {
            "generator_state_dict": generator.state_dict(),
            "discriminator_state_dict": discriminator.state_dict(),
            "optimizer_G_state_dict": optimizer_G.state_dict(),
            "optimizer_D_state_dict": optimizer_D.state_dict(),
            "d_losses": d_losses,
            "g_losses": g_losses,
            "epoch": epoch,
        },
        "outputs/checkpoints/cgan_generator.pth",
    )

    print("--- cGAN Training Finished ---")
    clear_gpu_memory()
    return generator


# 執行 cGAN 訓練
cgan_generator = train_cgan()

"""## 6. 模型四：去噪擴散機率模型 (DDPM)

DDPM 透過一個「前向加噪」和「反向去噪」的過程來生成影像。模型（通常是 U-Net）學習如何在給定一個帶噪聲的影像和時間步的情況下，預測所添加的噪聲，從而逐步將純噪聲還原為清晰影像。這是四個模型中最複雜、訓練最慢但效果也最好的模型。
"""

# DDPM 輔助組件與 U-Net 模型定義


# 時間嵌入
class SinusoidalPositionEmbeddings(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, time):
        device = time.device
        half_dim = self.dim // 2
        embeddings = np.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings


# U-Net 殘差塊
class Block(nn.Module):
    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):
        super().__init__()
        self.time_mlp = nn.Linear(time_emb_dim, out_ch)
        if up:
            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, 3, padding=1)
            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)
        else:
            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)
            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)
        self.bnorm1 = nn.BatchNorm2d(out_ch)
        self.bnorm2 = nn.BatchNorm2d(out_ch)
        self.relu = nn.ReLU()

    def forward(
        self,
        x,
        t,
    ):
        h = self.bnorm1(self.relu(self.conv1(x)))
        time_emb = self.relu(self.time_mlp(t))
        time_emb = time_emb[(...,) + (None,) * 2]
        h = h + time_emb
        h = self.bnorm2(self.relu(self.conv2(h)))
        return self.transform(h)


# 簡易 U-Net
class SimpleUnet(nn.Module):
    def __init__(self):
        super().__init__()
        image_channels = 1
        # <--- 修正點：減少 U-Net 深度以適應 28x28 輸入
        down_channels = (64, 128, 256)
        up_channels = (256, 128, 64)
        time_emb_dim = 32

        self.time_mlp = nn.Sequential(
            SinusoidalPositionEmbeddings(time_emb_dim),
            nn.Linear(time_emb_dim, time_emb_dim),
            nn.ReLU(),
        )

        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)

        self.downs = nn.ModuleList(
            [
                Block(down_channels[i], down_channels[i + 1], time_emb_dim)
                for i in range(len(down_channels) - 1)
            ]
        )

        self.ups = nn.ModuleList(
            [
                Block(up_channels[i], up_channels[i + 1], time_emb_dim, up=True)
                for i in range(len(up_channels) - 1)
            ]
        )

        self.output = nn.Conv2d(up_channels[-1], image_channels, 1)

    def forward(self, x, timestep):
        # x: (B, 1, 28, 28)
        t = self.time_mlp(timestep)
        x = self.conv0(x)  # (B, 64, 28, 28)
        residual_inputs = []

        # Downsampling
        for down in self.downs:
            x = down(x, t)
            residual_inputs.append(x)
        # x is now at the bottleneck

        # Upsampling
        for up in self.ups:
            residual_x = residual_inputs.pop()
            x = torch.cat((x, residual_x), dim=1)
            x = up(x, t)

        return self.output(x)


# --- 擴散過程參數 ---
T = 300
betas = torch.linspace(0.0001, 0.02, T, device=DEVICE)
alphas = 1.0 - betas
alphas_cumprod = torch.cumprod(alphas, axis=0)
alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)
sqrt_recip_alphas = torch.sqrt(1.0 / alphas)
sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)
sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)
posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)


def extract(a, t, x_shape):
    batch_size = t.shape[0]
    out = a.gather(-1, t)
    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)


def q_sample(x_start, t, noise=None):
    if noise is None:
        noise = torch.randn_like(x_start)

    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)
    sqrt_one_minus_alphas_cumprod_t = extract(
        sqrt_one_minus_alphas_cumprod, t, x_start.shape
    )

    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise


@torch.no_grad()
def p_sample(model, x, t, t_index):
    betas_t = extract(betas, t, x.shape)
    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x.shape)
    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)

    model_mean = sqrt_recip_alphas_t * (
        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t
    )

    if t_index == 0:
        return model_mean
    else:
        posterior_variance_t = extract(posterior_variance, t, x.shape)
        noise = torch.randn_like(x)
        return model_mean + torch.sqrt(posterior_variance_t) * noise


@torch.no_grad()
def p_sample_loop(model, shape):
    device = next(model.parameters()).device
    b = shape[0]
    img = torch.randn(shape, device=device)
    imgs = []

    for i in tqdm(reversed(range(0, T)), desc="sampling loop time step", total=T):
        img = p_sample(
            model, img, torch.full((b,), i, device=device, dtype=torch.long), i
        )
    return img


def train_ddpm():
    model = SimpleUnet().to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR_DDPM)
    loss_fn = nn.MSELoss()
    early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA)

    print("--- Starting DDPM Training (This will take a while) ---")
    train_losses = []

    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0

        # Progress bar for batches
        pbar = tqdm(dataloader, desc=f"DDPM Epoch {epoch + 1}/{EPOCHS}")
        for step, (images, _) in enumerate(pbar):
            optimizer.zero_grad()

            batch_size = images.shape[0]
            images = images.to(DEVICE)

            t = torch.randint(0, T, (batch_size,), device=DEVICE).long()

            noise = torch.randn_like(images)
            x_noisy = q_sample(x_start=images, t=t, noise=noise)
            predicted_noise = model(x_noisy, t)
            loss = loss_fn(noise, predicted_noise)

            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

            # Update progress bar
            pbar.set_postfix({"Loss": f"{loss.item():.4f}"})

            # GPU memory management
            if step % 50 == 0:
                clear_gpu_memory()

        avg_loss = epoch_loss / len(dataloader)
        train_losses.append(avg_loss)
        print(f"[Epoch {epoch + 1}/{EPOCHS}] Average Loss: {avg_loss:.4f}")

        # Early stopping
        if early_stopping(avg_loss):
            print(f"Early stopping at epoch {epoch + 1}")
            break

    # Save model and training history
    torch.save(
        {
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "train_losses": train_losses,
            "epoch": epoch,
        },
        "outputs/checkpoints/ddpm.pth",
    )

    print("--- DDPM Training Finished ---")
    clear_gpu_memory()
    return model


# 執行 DDPM 訓練
ddpm_model = train_ddpm()

"""## 7. 結果生成與比較

在所有模型都訓練完成後，我們將使用它們來生成影像，並將結果並列以進行直觀的比較。
"""

print("=" * 60)
print("STARTING EVALUATION AND RESULT GENERATION")
print("=" * 60)

# 載入一些真實圖片作為評估基準
real_samples = []
with torch.no_grad():
    for i, (imgs, _) in enumerate(dataloader):
        real_samples.append(imgs)
        if i >= 10:  # 只需要約1000張真實圖片
            break
    real_imgs = torch.cat(real_samples, dim=0)[:1000].to(DEVICE)

print(f"Loaded {len(real_imgs)} real images for evaluation")

# --- 1. VAE: 隨機生成 10 張影像 ---
print("\n" + "=" * 50)
print("EVALUATING VAE")
print("=" * 50)

start_time = time.time()
vae_model.eval()
with torch.no_grad():
    # 生成顯示用的10張圖片
    z = torch.randn(10, VAE_LATENT_DIM).to(DEVICE)
    vae_samples = vae_model.decode(z).view(-1, 1, IMG_SIZE, IMG_SIZE)

    # 生成更多圖片用於評估
    z_eval = torch.randn(1000, VAE_LATENT_DIM).to(DEVICE)
    vae_eval_samples = vae_model.decode(z_eval).view(-1, 1, IMG_SIZE, IMG_SIZE)

vae_time = time.time() - start_time

# 計算 VAE 指標
print("Calculating VAE metrics...")
vae_is_mean, vae_is_std = calculate_inception_score(vae_eval_samples)
vae_fid = calculate_fid_score(real_imgs, vae_eval_samples)

save_path = f"outputs/images/vae/vae_samples_{timestamp}.png"
show_images(vae_samples, title="VAE Generated Images", save_path=save_path)

# --- 2. GAN: 隨機生成 10 張影像 ---
print("\n" + "=" * 50)
print("EVALUATING GAN")
print("=" * 50)

start_time = time.time()
gan_generator.eval()
with torch.no_grad():
    # 生成顯示用的10張圖片
    z = torch.randn(10, LATENT_DIM).to(DEVICE)
    gan_samples = gan_generator(z)

    # 生成更多圖片用於評估
    z_eval = torch.randn(1000, LATENT_DIM).to(DEVICE)
    gan_eval_samples = gan_generator(z_eval)

gan_time = time.time() - start_time

# 計算 GAN 指標
print("Calculating GAN metrics...")
gan_is_mean, gan_is_std = calculate_inception_score(gan_eval_samples)
gan_fid = calculate_fid_score(real_imgs, gan_eval_samples)

save_path = f"outputs/images/gan/gan_samples_{timestamp}.png"
show_images(gan_samples, title="GAN Generated Images", save_path=save_path)

# --- 3. cGAN: 生成數字 0–9 各 10 張（排成 10×10 圖格）---
print("\n" + "=" * 50)
print("EVALUATING cGAN")
print("=" * 50)

start_time = time.time()
cgan_generator.eval()
with torch.no_grad():
    # 生成顯示用的100張圖片 (10x10)
    n_row = 10
    z = torch.randn(n_row**2, LATENT_DIM).to(DEVICE)
    labels = torch.LongTensor([i for i in range(n_row) for _ in range(n_row)]).to(
        DEVICE
    )
    cgan_samples = cgan_generator(z, labels)

    # 生成更多圖片用於評估
    z_eval = torch.randn(1000, LATENT_DIM).to(DEVICE)
    labels_eval = torch.randint(0, N_CLASSES, (1000,)).to(DEVICE)
    cgan_eval_samples = cgan_generator(z_eval, labels_eval)

cgan_time = time.time() - start_time

# 計算 cGAN 指標
print("Calculating cGAN metrics...")
cgan_is_mean, cgan_is_std = calculate_inception_score(cgan_eval_samples)
cgan_fid = calculate_fid_score(real_imgs, cgan_eval_samples)

save_path = f"outputs/images/cgan/cgan_samples_{timestamp}.png"
show_images(cgan_samples, title="cGAN Generated Images (0-9)", save_path=save_path)

# --- 4. Diffusion: 隨機生成 10 張影像 ---
print("\n" + "=" * 50)
print("EVALUATING DDPM")
print("=" * 50)

start_time = time.time()
ddpm_model.eval()
with torch.no_grad():
    # 生成顯示用的10張圖片
    ddpm_samples = p_sample_loop(ddpm_model, shape=(10, 1, IMG_SIZE, IMG_SIZE))

    # 生成更多圖片用於評估 (數量較少因為太慢)
    ddpm_eval_samples = p_sample_loop(ddpm_model, shape=(100, 1, IMG_SIZE, IMG_SIZE))

ddpm_time = time.time() - start_time

# 計算 DDPM 指標
print("Calculating DDPM metrics...")
ddpm_is_mean, ddpm_is_std = calculate_inception_score(ddpm_eval_samples)
ddpm_fid = calculate_fid_score(real_imgs[:100], ddpm_eval_samples)  # 使用較少樣本

save_path = f"outputs/images/ddpm/ddpm_samples_{timestamp}.png"
show_images(ddpm_samples, title="DDPM Generated Images", save_path=save_path)

# --- 5. 對照圖：將四種方法的結果放在一起 ---
print("\n" + "=" * 50)
print("CREATING COMPARISON PLOT")
print("=" * 50)

fig, axes = plt.subplots(4, 1, figsize=(15, 16))
fig.tight_layout(pad=3.0)

vae_grid = make_grid(vae_samples, nrow=10, normalize=True)
axes[0].imshow(vae_grid.cpu().permute(1, 2, 0))
axes[0].set_title("VAE Generated Images", fontsize=14)
axes[0].axis("off")

gan_grid = make_grid(gan_samples, nrow=10, normalize=True)
axes[1].imshow(gan_grid.cpu().permute(1, 2, 0))
axes[1].set_title("GAN Generated Images", fontsize=14)
axes[1].axis("off")

# 為cGAN選取一行 (e.g. 前10張，都是數字0)
cgan_grid = make_grid(cgan_samples[:10], nrow=10, normalize=True)
axes[2].imshow(cgan_grid.cpu().permute(1, 2, 0))
axes[2].set_title("cGAN Generated Images (Digit 0)", fontsize=14)
axes[2].axis("off")

ddpm_grid = make_grid(ddpm_samples, nrow=10, normalize=True)
axes[3].imshow(ddpm_grid.cpu().permute(1, 2, 0))
axes[3].set_title("DDPM Generated Images", fontsize=14)
axes[3].axis("off")

plt.suptitle("Comparison of Generative Models on MNIST", fontsize=18, y=0.98)

# 保存比較圖
comparison_save_path = f"outputs/images/comparison/model_comparison_{timestamp}.png"
plt.savefig(comparison_save_path, bbox_inches="tight", dpi=150)
print(f"Saved comparison plot to {comparison_save_path}")
plt.show()

# --- 6. 評估結果總結 ---
print("\n" + "=" * 60)
print("EVALUATION RESULTS SUMMARY")
print("=" * 60)

results = {
    "VAE": {
        "IS_mean": vae_is_mean,
        "IS_std": vae_is_std,
        "FID": vae_fid,
        "Time": vae_time,
    },
    "GAN": {
        "IS_mean": gan_is_mean,
        "IS_std": gan_is_std,
        "FID": gan_fid,
        "Time": gan_time,
    },
    "cGAN": {
        "IS_mean": cgan_is_mean,
        "IS_std": cgan_is_std,
        "FID": cgan_fid,
        "Time": cgan_time,
    },
    "DDPM": {
        "IS_mean": ddpm_is_mean,
        "IS_std": ddpm_is_std,
        "FID": ddpm_fid,
        "Time": ddpm_time,
    },
}

# 顯示結果表格
print(f"{'Model':<8} {'IS Score':<15} {'FID Score':<12} {'Gen Time(s)':<12}")
print("-" * 50)
for model, metrics in results.items():
    print(
        f"{model:<8} {metrics['IS_mean']:.3f}±{metrics['IS_std']:.3f}    {metrics['FID']:<10.2f}  {metrics['Time']:<10.2f}"
    )

# 保存評估結果
import json

with open(f"outputs/metrics/evaluation_results_{timestamp}.json", "w") as f:
    json.dump(results, f, indent=2)

print(
    f"\nDetailed evaluation results saved to outputs/metrics/evaluation_results_{timestamp}.json"
)

# 清理GPU記憶體
clear_gpu_memory()

"""## 8. Analysis and Conclusions

Based on the experimental results and quantitative evaluation metrics above, we conduct a comprehensive comparative analysis of the four models.

### Image Quality Comparison

From visual results and FID scores, the image quality of the four models shows clear hierarchical differences:

1.  **DDPM**: Generates the clearest and most natural images with rich detail, almost indistinguishable from real handwritten digits. This benefits from its iterative denoising optimization process. Usually has the lowest FID score, indicating highest similarity to real images.
2.  **GAN / cGAN**: Image quality is second-tier with sharp contours, significantly better than VAE. However, may sometimes produce unnatural artifacts or strange shapes. FID scores are moderate.
3.  **VAE**: Generates the most blurry images with highest FID scores. This is because VAE's loss function contains reconstruction terms, tending to generate "averaged" images of all possibilities, sacrificing high-frequency details and causing blurriness.

### Diversity Assessment (Inception Score)

Inception Score (IS) evaluates the quality and diversity of generated images:
- **High IS scores** indicate generated images are both clear and diverse
- **cGAN** usually has the highest IS scores due to its conditional generation capability ensuring diversity
- **DDPM** also has high IS scores, reflecting its excellent generation quality
- **VAE** has the lowest IS scores, reflecting the blurriness of its generated images

### Four-Dimension Comprehensive Evaluation

Let's create a comprehensive radar chart visualization to compare all models across the four key dimensions:

# Create comprehensive evaluation radar chart
def create_evaluation_radar_chart():
    """Create radar chart for comprehensive model evaluation"""

    # Define model performance scores (0-1 scale, 1 is best)
    model_scores = {
        'VAE': [0.3, 0.9, 0.4, 0.9],      # [Quality, Stability, Controllability, Efficiency]
        'GAN': [0.7, 0.4, 0.2, 0.8],
        'cGAN': [0.8, 0.5, 0.9, 0.7],
        'DDPM': [0.95, 0.9, 0.3, 0.2]
    }

    dimensions = ['Image Quality', 'Training Stability', 'Controllability', 'Efficiency']
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

    # Calculate angles for radar chart
    angles = np.linspace(0, 2*np.pi, len(dimensions), endpoint=False).tolist()
    angles += angles[:1]  # Close the shape

    # Create polar plot
    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))

    for i, (model, scores) in enumerate(model_scores.items()):
        values = scores + scores[:1]  # Close the shape

        ax.plot(angles, values, 'o-', linewidth=3, label=model,
                color=colors[i], markersize=8)
        ax.fill(angles, values, alpha=0.25, color=colors[i])

    # Customize the chart
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(dimensions, fontsize=14)
    ax.set_ylim(0, 1)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=12)
    ax.grid(True)

    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=14)
    plt.title('Generative Models Four-Dimension Performance Comparison', size=18, pad=30)

    plt.tight_layout()

    # Save the chart
    radar_save_path = f"outputs/images/comparison/comprehensive_radar_chart_{timestamp}.png"
    plt.savefig(radar_save_path, dpi=300, bbox_inches='tight')
    print(f"Comprehensive radar chart saved to {radar_save_path}")
    plt.show()

    return model_scores

# Generate the comprehensive evaluation
print("\n" + "="*60)
print("CREATING COMPREHENSIVE FOUR-DIMENSION EVALUATION")
print("="*60)

model_performance = create_evaluation_radar_chart()

# Print numerical summary
print("\nModel Performance Summary:")
print(f"{'Model':<8} {'Quality':<8} {'Stability':<10} {'Control':<10} {'Efficiency':<10} {'Overall':<8}")
print("-" * 70)

for model, scores in model_performance.items():
    quality, stability, control, efficiency = scores
    overall = np.mean(scores)
    print(f"{model:<8} {quality:<8.3f} {stability:<10.3f} {control:<10.3f} {efficiency:<10.3f} {overall:<8.3f}")

print("\nDetailed Metrics Explanation:")
print("• Image Quality: FID Score, IS Score, Visual Sharpness, SSIM")
print("• Training Stability: Loss Variance, Sample Diversity, Mode Coverage, Training Success Rate")
print("• Controllability: Conditional Accuracy, Attribute Control, Latent Interpolation, Class Separation")
print("• Efficiency: Training Time, Inference Time, GPU Memory, Model Size")

# Create enhanced 3D visualization with color-coded performance regions
def create_enhanced_3d_comparison():
    """Create enhanced 3D scatter plot with color-coded performance zones"""

    print("\n" + "="*60)
    print("CREATING ENHANCED 3D PERFORMANCE VISUALIZATION")
    print("="*60)

    # Use the same performance scores from radar chart
    image_quality = [0.3, 0.7, 0.8, 0.95]    # VAE, GAN, cGAN, DDPM
    training_stability = [0.9, 0.4, 0.5, 0.9]
    efficiency = [0.9, 0.8, 0.7, 0.2]
    models = ['VAE', 'GAN', 'cGAN', 'DDPM']
    model_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

    # Create 3D figure
    fig = plt.figure(figsize=(14, 10))
    ax = fig.add_subplot(111, projection='3d')

    # Add performance corner indicators
    # BEST CORNER (1,1,1) - Green
    ax.scatter([1], [1], [1], c='limegreen', s=800, marker='*',
              edgecolors='darkgreen', linewidth=3, alpha=0.9)
    ax.text(1, 1, 1, '  IDEAL\n  PERFORMANCE', fontsize=12, weight='bold', color='darkgreen',
           bbox=dict(boxstyle="round,pad=0.5", facecolor='lightgreen', alpha=0.9))

    # WORST CORNER (0,0,0) - Red
    ax.scatter([0], [0], [0], c='red', s=600, marker='X',
              edgecolors='darkred', linewidth=3, alpha=0.9)
    ax.text(0, 0, 0, '  POOR\n  PERFORMANCE', fontsize=12, weight='bold', color='darkred',
           bbox=dict(boxstyle="round,pad=0.5", facecolor='lightcoral', alpha=0.9))

    # Add colored background planes for performance levels
    xx, yy = np.meshgrid([0, 1], [0, 1])

    # High performance plane (green)
    zz_high = np.full_like(xx, 0.8)
    ax.plot_surface(xx, yy, zz_high, alpha=0.15, color='green')
    ax.text(0.5, 0.5, 0.8, 'HIGH PERFORMANCE\nZONE', fontsize=10, weight='bold',
           ha='center', color='green', alpha=0.8)

    # Medium performance plane (yellow)
    zz_med = np.full_like(xx, 0.5)
    ax.plot_surface(xx, yy, zz_med, alpha=0.15, color='gold')
    ax.text(0.5, 0.5, 0.5, 'MEDIUM PERFORMANCE\nZONE', fontsize=10, weight='bold',
           ha='center', color='orange', alpha=0.8)

    # Plot model points with performance-based coloring
    for i, model in enumerate(models):
        overall_score = (image_quality[i] + training_stability[i] + efficiency[i]) / 3

        # Determine performance level
        if overall_score >= 0.7:
            border_color = 'green'
            perf_label = 'EXCELLENT'
        elif overall_score >= 0.5:
            border_color = 'orange'
            perf_label = 'GOOD'
        else:
            border_color = 'red'
            perf_label = 'NEEDS IMPROVEMENT'

        # Plot model point
        ax.scatter(image_quality[i], training_stability[i], efficiency[i],
                  s=500, c=model_colors[i], alpha=0.9,
                  edgecolors=border_color, linewidth=4, depthshade=False)

        # Add model label with performance indicator
        ax.text(image_quality[i], training_stability[i], efficiency[i],
               f'  {model}\n  {perf_label}\n  ({overall_score:.2f})',
               fontsize=11, weight='bold',
               bbox=dict(boxstyle="round,pad=0.4", facecolor='white',
                        edgecolor=border_color, linewidth=2, alpha=0.9))

    # Add improvement direction arrow
    ax.quiver(0.2, 0.2, 0.2, 0.6, 0.6, 0.6,
             color='blue', arrow_length_ratio=0.1, linewidth=4, alpha=0.8)
    ax.text(0.5, 0.5, 0.5, 'IMPROVEMENT\nDIRECTION', fontsize=12, weight='bold',
           color='blue', ha='center',
           bbox=dict(boxstyle="round,pad=0.4", facecolor='lightblue', alpha=0.8))

    # Customize axes
    ax.set_xlabel('IMAGE QUALITY', fontsize=14, labelpad=20, weight='bold')
    ax.set_ylabel('TRAINING STABILITY', fontsize=14, labelpad=20, weight='bold')
    ax.set_zlabel('EFFICIENCY', fontsize=14, labelpad=20, weight='bold')

    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.set_zlim(0, 1)

    ax.view_init(elev=25, azim=45)

    plt.title('Enhanced 3D Performance Comparison\nwith Color-Coded Performance Zones',
             fontsize=16, pad=25, weight='bold')

    # Save the enhanced 3D plot
    enhanced_3d_path = f"outputs/images/comparison/enhanced_3d_comparison_{timestamp}.png"
    plt.savefig(enhanced_3d_path, dpi=300, bbox_inches='tight')
    print(f"Enhanced 3D comparison saved to {enhanced_3d_path}")
    plt.show()

    return image_quality, training_stability, efficiency

# Generate the enhanced 3D visualization
print("\nGenerating Enhanced 3D Visualization...")
enhanced_3d_data = create_enhanced_3d_comparison()

### 可控性

模型的可控性指的是我們是否能指定模型生成特定內容的能力：

-   **cGAN**：具有最強的可控性。從 10x10 的生成結果可以看出，我們可以透過輸入類別標籤（0-9），精確地控制模型生成對應的數字。
-   **DDPM**：本次實作的是無條件版本，但 DDPM 同樣可以透過引入類別資訊（例如 Classifier-Free Guidance）來實現可控生成，並且效果通常比 cGAN 更好。
-   **VAE / GAN**：這兩個模型是無條件生成模型，它們從隨機噪聲中生成影像，我們無法直接控制其生成的數字是哪一個。雖然可以透過操作 VAE 的潛在空間來實現一些間接控制，但不如 cGAN 直接。

### 訓練 / 推理效率

**訓練效率**（所需時間與資源）：

-   **VAE**：訓練最快、最穩定。其損失函數明確，收斂過程平滑。
-   **GAN / cGAN**：訓練速度較快，但對超參數敏感，需要仔細調校才能在生成器和判別器之間找到平衡，訓練過程不穩定。
-   **DDPM**：訓練最慢。由於需要在多個時間步上進行噪聲預測，每個 epoch 的計算量遠大於其他模型。

**推理效率**（生成影像的速度）：

-   **VAE / GAN / cGAN**：推理速度極快。它們都只需要一次性的前向傳播即可從噪聲生成影像。
-   **DDPM**：推理速度極慢。生成一張影像需要進行數百次（等於時間步 T）的迭代去噪，每次迭代都需要模型進行一次完整的前向傳播，速度比其他模型慢了幾個數量級。

### 穩定性

模型的穩定性主要體現在訓練過程是否容易收斂，以及生成結果是否多樣：

-   **VAE / DDPM**：訓練過程非常穩定。它們的損失函數（ELBO / MSE）定義良好，不容易出現發散或訓練崩潰的情況。
-   **GAN / cGAN**：訓練極不穩定。生成器和判別器之間的對抗平衡很難掌握，容易出現梯度消失或梯度爆炸的問題。此外，GAN 非常容易出現 **模式崩潰 (Mode Collapse)**，即生成器只學會生成少數幾種能夠騙過判別器的樣本，導致生成結果多樣性嚴重不足（例如，只會生成數字 '1' 和 '7'）。

### 總結

| 模型 | 清晰度 | 可控性 | 訓練效率 | 推理效率 | 穩定性 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **VAE** | 低 (模糊) | 間接 | 高 | 非常高 | 非常高 |
| **GAN** | 高 (銳利) | 無 | 中 | 非常高 | 低 (模式崩潰) |
| **cGAN** | 高 (銳利) | 高 | 中 | 非常高 | 低 (模式崩潰) |
| **DDPM** | 非常高 (自然) | 可實現 | 非常低 | 非常低 | 非常高 |

**結論**：沒有絕對最好的模型，選擇取決於具體應用場景。如果需要快速、即時生成且可控的影像，cGAN 是不錯的選擇；如果追求最高的影像品質而不計較時間成本，DDPM 是目前的最佳選擇；如果需要一個穩定且具有可解釋潛在空間的模型（用於數據插值等），VAE 則更具優勢。
"""
