{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/YOUR_USERNAME/MNIST_COMP/blob/main/Evaluation_and_Visualization_Only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# MNIST Generative Models - Evaluation and Visualization Only\n",
    "\n",
    "This notebook loads pre-trained model checkpoints and performs evaluation and visualization without training.\n",
    "\n",
    "**Use this when you:**\n",
    "- Have already trained models and saved checkpoints\n",
    "- Want to create new visualizations\n",
    "- Need to calculate additional metrics\n",
    "- Want to generate more samples\n",
    "\n",
    "**Requirements:**\n",
    "- Checkpoint files (epoch 40) uploaded or in Google Drive\n",
    "- No GPU needed (but faster with GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Mount Checkpoints\n",
    "\n",
    "Choose ONE of the following methods to load your checkpoints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "method-description"
   },
   "source": [
    "### Method 1: Upload Files Directly (Recommended for < 50 MB)\n",
    "\n",
    "**Pros:** Simple, no Google Drive needed  \n",
    "**Cons:** Need to re-upload every session  \n",
    "**Best for:** Testing, one-time evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-files"
   },
   "outputs": [],
   "source": [
    "# Method 1: Upload checkpoint files directly\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Please upload your checkpoint files (epoch 40):\")\n",
    "print(\"  - vae_model_epoch_40.pth\")\n",
    "print(\"  - gan_generator_epoch_40.pth\")\n",
    "print(\"  - cgan_generator_epoch_40.pth\")\n",
    "print(\"  - ddpm_model_epoch_40.pth\")\n",
    "print(\"\\nClick 'Choose Files' below...\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Move uploaded files\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f'checkpoints/{filename}')\n",
    "    print(f\"Moved: {filename}\")\n",
    "\n",
    "print(\"\\nCheckpoints ready in: checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "method2-description"
   },
   "source": [
    "### Method 2: Mount Google Drive (Recommended for Regular Use)\n",
    "\n",
    "**Pros:** Persistent, no re-upload needed  \n",
    "**Cons:** Requires organizing files in Drive  \n",
    "**Best for:** Regular evaluation, multiple sessions\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Upload checkpoints to Google Drive (e.g., `My Drive/MNIST_Checkpoints/`)\n",
    "2. Run the cell below\n",
    "3. Grant Drive access when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Method 2: Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set path to your checkpoints in Drive\n",
    "# EDIT THIS PATH to match where you saved your checkpoints\n",
    "DRIVE_CHECKPOINT_PATH = '/content/drive/MyDrive/MNIST_Checkpoints'\n",
    "\n",
    "# Create symbolic link for easy access\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.symlink(DRIVE_CHECKPOINT_PATH, 'checkpoints')\n",
    "    print(f\"Linked checkpoints from: {DRIVE_CHECKPOINT_PATH}\")\n",
    "else:\n",
    "    print(\"Checkpoints folder already exists\")\n",
    "\n",
    "# Verify files\n",
    "print(\"\\nFiles found:\")\n",
    "for f in os.listdir('checkpoints'):\n",
    "    if f.endswith('.pth'):\n",
    "        size = os.path.getsize(f'checkpoints/{f}') / (1024*1024)\n",
    "        print(f\"  {f} ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "method3-description"
   },
   "source": [
    "### Method 3: Download from URL (Advanced)\n",
    "\n",
    "**Pros:** Automated, sharable  \n",
    "**Cons:** Need to host files somewhere  \n",
    "**Best for:** Shared workflows, GitHub releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-url"
   },
   "outputs": [],
   "source": [
    "# Method 3: Download from URL (e.g., GitHub releases, Dropbox, etc.)\n",
    "import os\n",
    "\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Example: Download from a public URL\n",
    "# Replace these URLs with your actual file locations\n",
    "urls = {\n",
    "    'vae_model_epoch_40.pth': 'YOUR_URL_HERE',\n",
    "    'gan_generator_epoch_40.pth': 'YOUR_URL_HERE',\n",
    "    'cgan_generator_epoch_40.pth': 'YOUR_URL_HERE',\n",
    "    'ddpm_model_epoch_40.pth': 'YOUR_URL_HERE'\n",
    "}\n",
    "\n",
    "# Uncomment and edit URLs above, then run:\n",
    "# for filename, url in urls.items():\n",
    "#     !wget -O checkpoints/{filename} {url}\n",
    "#     print(f\"Downloaded: {filename}\")\n",
    "\n",
    "print(\"Note: Edit the URLs above and uncomment the code to use this method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify-checkpoints"
   },
   "source": [
    "### Verify Checkpoints\n",
    "\n",
    "Run this cell to verify your checkpoints are accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Checking for checkpoint files...\\n\")\n",
    "\n",
    "required_files = [\n",
    "    'vae_model_epoch_40.pth',\n",
    "    'gan_generator_epoch_40.pth',\n",
    "    'cgan_generator_epoch_40.pth',\n",
    "    'ddpm_model_epoch_40.pth'\n",
    "]\n",
    "\n",
    "all_found = True\n",
    "for filename in required_files:\n",
    "    path = f'checkpoints/{filename}'\n",
    "    if os.path.exists(path):\n",
    "        size = os.path.getsize(path) / (1024*1024)\n",
    "        print(f\"Found: {filename} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"MISSING: {filename}\")\n",
    "        all_found = False\n",
    "\n",
    "if all_found:\n",
    "    print(\"\\nAll checkpoints found! Ready to proceed.\")\n",
    "else:\n",
    "    print(\"\\nSome checkpoints are missing. Please upload them using one of the methods above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-dependencies"
   },
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "from scipy.stats import entropy\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('outputs/visualizations', exist_ok=True)\n",
    "\n",
    "print(\"\\nAll dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-architectures"
   },
   "source": [
    "## 3. Model Architectures\n",
    "\n",
    "Define model architectures (must match training code exactly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "models"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder\"\"\"\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(128 * 3 * 3, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128 * 3 * 3, latent_dim)\n",
    "        \n",
    "        self.decoder_input = nn.Linear(latent_dim, 128 * 3 * 3)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = self.decoder_input(z)\n",
    "        h = h.view(-1, 128, 3, 3)\n",
    "        return self.decoder(h)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"GAN Generator\"\"\"\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z).view(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    \"\"\"Conditional GAN Generator\"\"\"\n",
    "    def __init__(self, latent_dim=100, num_classes=10):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        return self.model(gen_input).view(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"DDPM UNet\"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, time_emb_dim=32):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256)\n",
    "        )\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, 3, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 64, 3, padding=1)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, out_channels, 3, padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000 ** (torch.arange(0, channels, 2, device=t.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "    \n",
    "    def forward(self, x, timestep):\n",
    "        t = self.pos_encoding(timestep.float().unsqueeze(-1), 32)\n",
    "        t = self.time_mlp(t)\n",
    "        \n",
    "        x1 = self.relu(self.conv1(x))\n",
    "        x2 = self.relu(self.conv2(x1))\n",
    "        x3 = self.relu(self.conv3(x2))\n",
    "        \n",
    "        t = t.view(-1, 256, 1, 1).expand(-1, -1, x3.shape[2], x3.shape[3])\n",
    "        x3 = x3 + t\n",
    "        \n",
    "        x = self.relu(self.upconv3(x3))\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.relu(self.upconv2(x))\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.upconv1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Model architectures defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-checkpoints"
   },
   "source": [
    "## 4. Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_path):\n",
    "    \"\"\"Load model from checkpoint\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Warning: {checkpoint_path} not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Loaded: {os.path.basename(checkpoint_path)}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {os.path.basename(checkpoint_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Loading models from checkpoints...\\n\")\n",
    "\n",
    "# Load all models\n",
    "vae_model = load_checkpoint(VAE(latent_dim=20), 'checkpoints/vae_model_epoch_40.pth')\n",
    "gan_model = load_checkpoint(Generator(latent_dim=100), 'checkpoints/gan_generator_epoch_40.pth')\n",
    "cgan_model = load_checkpoint(ConditionalGenerator(latent_dim=100), 'checkpoints/cgan_generator_epoch_40.pth')\n",
    "ddpm_model = load_checkpoint(UNet(), 'checkpoints/ddpm_model_epoch_40.pth')\n",
    "\n",
    "models = {\n",
    "    'vae': vae_model,\n",
    "    'gan': gan_model,\n",
    "    'cgan': cgan_model,\n",
    "    'ddpm': ddpm_model\n",
    "}\n",
    "\n",
    "loaded_count = sum(1 for m in models.values() if m is not None)\n",
    "print(f\"\\nLoaded {loaded_count}/4 models successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate-samples"
   },
   "source": [
    "## 5. Generate Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate"
   },
   "outputs": [],
   "source": [
    "print(\"Generating sample images...\\n\")\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "samples = {}\n",
    "\n",
    "# VAE\n",
    "if vae_model is not None:\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, 20).to(device)\n",
    "        samples['VAE'] = vae_model.decode(z).cpu()\n",
    "    print(f\"VAE: {num_samples} samples generated\")\n",
    "\n",
    "# GAN\n",
    "if gan_model is not None:\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, 100).to(device)\n",
    "        samples['GAN'] = gan_model(z).cpu()\n",
    "    print(f\"GAN: {num_samples} samples generated\")\n",
    "\n",
    "# cGAN (one sample per digit)\n",
    "if cgan_model is not None:\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, 100).to(device)\n",
    "        labels = torch.arange(num_samples).to(device)\n",
    "        samples['cGAN'] = cgan_model(z, labels).cpu()\n",
    "    print(f\"cGAN: {num_samples} samples generated (digits 0-9)\")\n",
    "\n",
    "# DDPM (simplified)\n",
    "if ddpm_model is not None:\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(num_samples, 1, 28, 28).to(device)\n",
    "        t = torch.zeros(num_samples).to(device)\n",
    "        noise_pred = ddpm_model(x, t)\n",
    "        samples['DDPM'] = (x - noise_pred * 0.1).cpu()\n",
    "    print(f\"DDPM: {num_samples} samples generated\")\n",
    "\n",
    "print(f\"\\nTotal: {len(samples)} model(s) ready for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "## 6. Visualization - Sample Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vis-grid"
   },
   "outputs": [],
   "source": [
    "# Create comparison grid\n",
    "fig, axes = plt.subplots(len(samples), 10, figsize=(20, 2*len(samples)))\n",
    "\n",
    "if len(samples) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, (model_name, images) in enumerate(samples.items()):\n",
    "    for j in range(10):\n",
    "        if len(samples) > 1:\n",
    "            ax = axes[i, j]\n",
    "        else:\n",
    "            ax = axes[j]\n",
    "        \n",
    "        ax.imshow(images[j].squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        if j == 0:\n",
    "            ax.set_ylabel(model_name, fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Generated Samples Comparison', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/visualizations/sample_grid.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: outputs/visualizations/sample_grid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom-generation"
   },
   "source": [
    "## 7. Custom Generation Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom-cgan"
   },
   "source": [
    "### Generate Specific Digits with cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgan-digits"
   },
   "outputs": [],
   "source": [
    "if cgan_model is not None:\n",
    "    # Generate 100 samples of digit \"7\"\n",
    "    target_digit = 7\n",
    "    num_samples = 100\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, 100).to(device)\n",
    "        labels = torch.full((num_samples,), target_digit).to(device)\n",
    "        digit_samples = cgan_model(z, labels).cpu()\n",
    "    \n",
    "    # Display first 25\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        axes[i//5, i%5].imshow(digit_samples[i].squeeze(), cmap='gray')\n",
    "        axes[i//5, i%5].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'100 Generated Samples of Digit {target_digit}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Generated {num_samples} samples of digit {target_digit}\")\n",
    "else:\n",
    "    print(\"cGAN model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "latent-interpolation"
   },
   "source": [
    "### Latent Space Interpolation (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vae-interpolation"
   },
   "outputs": [],
   "source": [
    "if vae_model is not None:\n",
    "    # Interpolate between two random latent vectors\n",
    "    z1 = torch.randn(1, 20).to(device)\n",
    "    z2 = torch.randn(1, 20).to(device)\n",
    "    \n",
    "    steps = 10\n",
    "    alphas = torch.linspace(0, 1, steps)\n",
    "    \n",
    "    interpolated = []\n",
    "    with torch.no_grad():\n",
    "        for alpha in alphas:\n",
    "            z = z1 * (1 - alpha) + z2 * alpha\n",
    "            img = vae_model.decode(z).cpu()\n",
    "            interpolated.append(img)\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, steps, figsize=(20, 2))\n",
    "    for i in range(steps):\n",
    "        axes[i].imshow(interpolated[i].squeeze(), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'{alphas[i]:.1f}', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('VAE Latent Space Interpolation', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Generated {steps} interpolation steps\")\n",
    "else:\n",
    "    print(\"VAE model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-results"
   },
   "source": [
    "## 8. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# Zip all visualizations\n",
    "!zip -r visualizations.zip outputs/visualizations/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('visualizations.zip')\n",
    "\n",
    "print(\"Downloaded: visualizations.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "- ✅ Multiple methods to load checkpoints (upload, Drive, URL)\n",
    "- ✅ Checkpoint verification\n",
    "- ✅ Model loading with error handling\n",
    "- ✅ Sample generation from all models\n",
    "- ✅ Visualization grids\n",
    "- ✅ Custom generation examples (specific digits, interpolation)\n",
    "- ✅ Easy result download\n",
    "\n",
    "**No training required** - just load checkpoints and generate!\n",
    "\n",
    "### Next Steps:\n",
    "- Modify generation parameters\n",
    "- Create custom visualizations\n",
    "- Generate samples for presentations\n",
    "- Experiment with latent space"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
